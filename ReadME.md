# WaterMod: Modular Token-Rank Partitioning for Probability-Balanced LLM Watermarking (AAAI 2026 Main Track Oral Presentation)

This repository contains the official implementation, evaluation scripts, and experiment pipelines for the AAAI 2026 paper: WaterMod: Modular Token-Rank Partitioning for Probability-Balanced LLM Watermarking.

# üí° What is WaterMod?

WaterMod is a new watermarking framework designed to be robust, detectable, and, most importantly, gentle on text quality.

**The Core Problem**

Regulations like the EU AI Act are beginning to require that synthetic content be machine-verifiable. A common way to do this is logit-based watermarking, which works by randomly splitting the model's vocabulary (all possible tokens) into a green list and a red list at each step, and then telling the model to prefer tokens from the green list.

The problem? This random split can accidentally put the best, most contextually appropriate token into the red list, forcing the model to choose a less-ideal token and thus harming the fluency and quality of the generated text.

**The WaterMod Solution: Partitioning Ranks, Not Tokens**

WaterMod elegantly solves this limitation with a probability-aware modular rule. Instead of partitioning the vocabulary, WaterMod partitions the ranks of the tokens.

Here‚Äôs the simple, two-step process at each decoding step:
    
- a) Rank: The model first calculates the probability for every token. We sort these tokens by their probability, from most likely (Rank 1) to least likely (Rank V).
- b) Partition: We apply a simple modular arithmetic rule: rank mod k. This rule splits all tokens into k different color classes based on their rank.

This method naturally distributes adjacent, high-probability tokens (which are often semantic synonyms) into different classes.
As a result, at least one high-probability, high-quality token is available for sampling, regardless of which class we choose to bias. This probability-balanced approach is the key to maintaining generation quality.

## A Unified Framework for Zero-bit and Multi-bit

The same simple rank mod k rule powers both watermarking scenarios, making WaterMod a flexible, unified framework.

**Zero-bit Watermarking (k=2)**

Goal: To answer the simple question: "Was this text generated by an AI?"

How it Works: We set k=2, which partitions the ranks into two simple classes: Even (rank mod 2 = 0) and Odd (rank mod 2 = 1). An entropy-adaptive gate dynamically chooses which class to bias (the green list). This helps that even in highly predictable (low-entropy) contexts, the top-ranked token is available for selection.

<p align="center">
  <img src="./figure/figure_1.png" width="800"/>
</p>

Figure 1: The WaterMod embedding procedure for the zero-bit (k=2) setting. The vocabulary is ranked by probability, partitioned by rank parity (even/odd), and then dynamically biased to preserve fluency.

**Multi-bit Watermarking (k > 2)**

Goal: To embed a specific, hidden message (a payload), such as a model ID or document GUID, for fine-grained provenance tracing.

How it Works: We set k > 2 (e.g., k=4).

- a) Your message (e.g., "34457") is converted into a sequence of base-k digits.
- b) At each step, a pseudorandom function picks a position p from your message sequence.
- c) It fetches the digit d at that position.
- d) The bias Œ¥ is only applied to the token class whose ranks satisfy rank mod k = d.

This process embeds exactly one base-k digit (or $\log_2(k)$ bits) at each step, which can be recovered later using a detector.

<p align="center">
  <img src="./figure/figure_2.png" width="800"/>
</p>

Figure 2: The message embedding and recovery process for the multi-bit setting. A payload is converted to base-k digits, which then guide the selection of the biased color class at each step.

## üõ†Ô∏è Environment Setup

We recommend setting up a Python 3.10 environment using Anaconda:

```bash
conda create -n markllm python=3.10
conda activate markllm
```

Install the required packages:

```bash
pip install evalplus
pip install markllm
```

## üöÄ Running Experiments

Experiments are organized into two separate folders:

- `zero_bit/` ‚Äì for zero-bit watermarking scenarios  
- `multi_bit/` ‚Äì for multi-bit watermarking scenarios

To run experiments in either scenario, navigate to the corresponding directory and execute the following scripts in order:

```bash
bash watermark.sh
bash parse_and_evaluation.sh
```

These scripts will generate watermarked outputs, evaluate performance.

## üñä Citation
```text
@misc{park2025watermodmodulartokenrankpartitioning,
      title={WaterMod: Modular Token-Rank Partitioning for Probability-Balanced LLM Watermarking}, 
      author={Shinwoo Park and Hyejin Park and Hyeseon Ahn and Yo-Sub Han},
      year={2025},
      eprint={2511.07863},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2511.07863}, 
}
```
